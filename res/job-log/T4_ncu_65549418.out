+ srun nvidia-smi
Thu Oct  6 16:53:06 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:C9:00.0 Off |                    0 |
| N/A   25C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
+ env
SLURM_NODELIST=sciml2102
REMOTEHOST=xmei-lpt-wifi.jlab.org
SLURM_LUSTRE_JOB_ID=sciml2102,xmei,65549418
SLURM_JOB_NAME=lstm-train
XDG_SESSION_ID=261024
SLURMD_NODENAME=sciml2102
SLURM_TOPOLOGY_ADDR=sciml2102
HOSTNAME=sciml2102
SLURM_PRIO_PROCESS=0
SLURM_NODE_ALIASES=(null)
HOST=ifarm1901.jlab.org
TERM=xterm-256color
SHELL=/bin/tcsh
SLURM_JOB_QOS=normal
TMPDIR=/tmp
SLURM_TOPOLOGY_ADDR_PATTERN=node
SSH_CLIENT=129.57.82.157 36798 22
OSNAME=Linux
QTDIR=/usr/lib64/qt-3.3
SLURM_JOB_GPUS=13
QTINC=/usr/lib64/qt-3.3/include
SSH_TTY=/dev/pts/216
SLURM_MEM_PER_CPU=4000
QT_GRAPHICSSYSTEM_CHECKED=1
ROCR_VISIBLE_DEVICES=0
SLURM_NNODES=1
GROUP=ITD
USER=xmei
LS_COLORS=rs=0:di=38;5;27:ln=38;5;51:mh=44;38;5;15:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=05;48;5;232;38;5;15:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;34:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.tzo=38;5;9:*.t7z=38;5;9:*.zip=38;5;9:*.z=38;5;9:*.Z=38;5;9:*.dz=38;5;9:*.gz=38;5;9:*.lrz=38;5;9:*.lz=38;5;9:*.lzo=38;5;9:*.xz=38;5;9:*.bz2=38;5;9:*.bz=38;5;9:*.tbz=38;5;9:*.tbz2=38;5;9:*.tz=38;5;9:*.deb=38;5;9:*.rpm=38;5;9:*.jar=38;5;9:*.war=38;5;9:*.ear=38;5;9:*.sar=38;5;9:*.rar=38;5;9:*.alz=38;5;9:*.ace=38;5;9:*.zoo=38;5;9:*.cpio=38;5;9:*.7z=38;5;9:*.rz=38;5;9:*.cab=38;5;9:*.jpg=38;5;13:*.jpeg=38;5;13:*.gif=38;5;13:*.bmp=38;5;13:*.pbm=38;5;13:*.pgm=38;5;13:*.ppm=38;5;13:*.tga=38;5;13:*.xbm=38;5;13:*.xpm=38;5;13:*.tif=38;5;13:*.tiff=38;5;13:*.png=38;5;13:*.svg=38;5;13:*.svgz=38;5;13:*.mng=38;5;13:*.pcx=38;5;13:*.mov=38;5;13:*.mpg=38;5;13:*.mpeg=38;5;13:*.m2v=38;5;13:*.mkv=38;5;13:*.webm=38;5;13:*.ogm=38;5;13:*.mp4=38;5;13:*.m4v=38;5;13:*.mp4v=38;5;13:*.vob=38;5;13:*.qt=38;5;13:*.nuv=38;5;13:*.wmv=38;5;13:*.asf=38;5;13:*.rm=38;5;13:*.rmvb=38;5;13:*.flc=38;5;13:*.avi=38;5;13:*.fli=38;5;13:*.flv=38;5;13:*.gl=38;5;13:*.dl=38;5;13:*.xcf=38;5;13:*.xwd=38;5;13:*.yuv=38;5;13:*.cgm=38;5;13:*.emf=38;5;13:*.axv=38;5;13:*.anx=38;5;13:*.ogv=38;5;13:*.ogx=38;5;13:*.aac=38;5;45:*.au=38;5;45:*.flac=38;5;45:*.mid=38;5;45:*.midi=38;5;45:*.mka=38;5;45:*.mp3=38;5;45:*.mpc=38;5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.axa=38;5;45:*.oga=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:
SLURM_JOBID=65549418
HOSTTYPE=x86_64-linux
COLUMNS=110
SLURM_TASKS_PER_NODE=1
MAIL=/var/spool/mail/xmei
PATH=/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin
SLURM_WORKING_CLUSTER=scicomp:enpslurm21:6817:9216:101
SLURM_CONF=/etc/slurm/slurm.conf
SLURM_JOB_ID=65549418
SLURM_CPUS_PER_TASK=8
SLURM_JOB_USER=xmei
PWD=/home/xmei/projects/gluex-tracking-pytorch-lstm/T4
CUDA_VISIBLE_DEVICES=0
LANG=en_US.UTF-8
MODULEPATH=/usr/share/Modules/modulefiles:/etc/modulefiles
SLURM_JOB_UID=11066
LOADEDMODULES=
KDEDIRS=/usr
SLURM_NODEID=0
SLURM_SUBMIT_DIR=/w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/T4
SLURM_TASK_PID=53355
LINES=24
SLURM_CPUS_ON_NODE=8
SLURM_PROCID=0
ENVIRONMENT=BATCH
SLURM_JOB_NODELIST=sciml2102
PERL_HOMEDIR=0
SHLVL=2
HOME=/home/xmei
SLURM_LOCALID=0
OSTYPE=linux
SLURM_JOB_GID=761
SLURM_JOB_CPUS_PER_NODE=8
SLURM_CLUSTER_NAME=scicomp
SLURM_GTIDS=0
SLURM_SUBMIT_HOST=ifarm1901.jlab.org
SLURM_JOB_PARTITION=gpu
VENDOR=unknown
MACHTYPE=x86_64
LOGNAME=xmei
QTLIB=/usr/lib64/qt-3.3/lib
CVS_RSH=ssh
GPU_DEVICE_ORDINAL=0
SLURM_JOB_ACCOUNT=epsci
SSH_CONNECTION=129.57.82.157 36798 129.57.70.17 22
SLURM_JOB_NUM_NODES=1
MODULESHOME=/usr/share/Modules
LESSOPEN=||/usr/bin/lesspipe.sh %s
XDG_RUNTIME_DIR=/run/user/11066
OSREL=3.10.0-1160.71.1.el7.x86_64
QT_PLUGIN_PATH=/usr/lib64/kde4/plugins:/usr/lib/kde4/plugins
_=/usr/bin/env
+ pwd
/home/xmei/projects/gluex-tracking-pytorch-lstm/T4
+ source /etc/profile.d/modules.sh
++++ /bin/ps -p 53355 -ocomm=
+++ /bin/basename slurm_script
++ shell=slurm_script
++ '[' -f /usr/share/Modules/init/slurm_script ']'
++ . /usr/share/Modules/init/sh
+++ MODULESHOME=/usr/share/Modules
+++ export MODULESHOME
+++ '[' '' = '' ']'
+++ LOADEDMODULES=
+++ export LOADEDMODULES
+++ '[' /usr/share/Modules/modulefiles:/etc/modulefiles = '' ']'
+ module load python3
++ /usr/bin/modulecmd sh load python3
ModuleCmd_Load.c(213):ERROR:105: Unable to locate a modulefile for 'python3'
+ eval
+ pip3 install pandas numpy sklearn torch matplotlib
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: pandas in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (1.1.5)
Requirement already satisfied: numpy in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (1.19.5)
Requirement already satisfied: sklearn in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (0.0)
Requirement already satisfied: torch in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (1.10.2)
Requirement already satisfied: matplotlib in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (3.3.4)
Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas) (2021.3)
Requirement already satisfied: python-dateutil>=2.7.3 in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from pandas) (2.8.2)
Requirement already satisfied: scikit-learn in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from sklearn) (0.24.2)
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/site-packages (from torch) (4.1.1)
Requirement already satisfied: dataclasses in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from torch) (0.8)
Requirement already satisfied: cycler>=0.10 in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from matplotlib) (0.11.0)
Requirement already satisfied: kiwisolver>=1.0.1 in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from matplotlib) (1.3.1)
Requirement already satisfied: pillow>=6.2.0 in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from matplotlib) (8.4.0)
Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from matplotlib) (3.0.9)
Requirement already satisfied: six>=1.5 in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)
Requirement already satisfied: scipy>=0.19.1 in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.5.4)
Requirement already satisfied: threadpoolctl>=2.0.0 in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from scikit-learn->sklearn) (3.1.0)
Requirement already satisfied: joblib>=0.11 in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.1.0)
+ srun python3 Simplified_LSTM.py
Torch version 1.10.2+cu102
Training on cuda device
Epoch    1: loss=0.053595763205266087, mse=0.014823611259657163, val_loss=0.00905893504640805, val_mse=0.0002636370794499466, lr=0.0001
Epoch    2: loss=0.008372020633339854, mse=0.00025039622235141496, val_loss=0.007807941820147311, val_mse=0.0002401951289626609, lr=0.0001
Epoch    3: loss=0.007690891915529373, mse=0.00023939829266202004, val_loss=0.007383929440960203, val_mse=0.00023419067655814602, lr=0.0001
Epoch    4: loss=0.00743307178531662, mse=0.0002350120218874943, val_loss=0.00854782810508922, val_mse=0.00024380075756440107, lr=0.0001
Epoch    5: loss=0.007122489712969798, mse=0.00022973293044851328, val_loss=0.00803530197749639, val_mse=0.00023595809264871092, lr=0.0001
Epoch    6: loss=0.007042901542530779, mse=0.00022619367001188982, val_loss=0.00657541762535554, val_mse=0.00021897829683922563, lr=0.0001
Epoch    7: loss=0.006951632531071735, mse=0.00022124305176914253, val_loss=0.00732490582836958, val_mse=0.00022236548760791875, lr=0.0001
Epoch    8: loss=0.0068086742470306, mse=0.0002153063660323071, val_loss=0.006231742431241772, val_mse=0.00020633932417108406, lr=0.0001
Epoch    9: loss=0.0063463125450349284, mse=0.00020322091959884515, val_loss=0.006077325149161822, val_mse=0.0001944623522995485, lr=0.0001
Epoch   10: loss=0.0062823291740731904, mse=0.00018777808756319022, val_loss=0.006405110438257117, val_mse=0.00017771895180069321, lr=0.0001
Epoch   11: loss=0.0060600245316677235, mse=0.0001616370323347922, val_loss=0.00583725577870656, val_mse=0.0001444665807490617, lr=0.0001
Epoch   12: loss=0.006019524054069856, mse=0.00013421732417613855, val_loss=0.005839563889404232, val_mse=0.00012224620449534783, lr=0.0001
Epoch   13: loss=0.00552239437720984, mse=0.0001158876762459723, val_loss=0.005057272171704152, val_mse=0.00010979840373392862, lr=0.0001
Epoch   14: loss=0.005026309151446316, mse=0.00010635555183080991, val_loss=0.004748622951339895, val_mse=0.00010133801675430345, lr=0.0001
Epoch   15: loss=0.005271875266819853, mse=0.00010400137906121262, val_loss=0.005292086807243964, val_mse=0.0001032000590207239, lr=0.0001
Epoch   16: loss=0.0049667854071462235, mse=9.890004083031763e-05, val_loss=0.0044504937342945946, val_mse=9.394951072054079e-05, lr=0.0001
Epoch   17: loss=0.004896826784089349, mse=9.609730169795581e-05, val_loss=0.004578948339033851, val_mse=9.208855320689794e-05, lr=0.0001
Epoch   18: loss=0.004811525016588352, mse=9.392341030391037e-05, val_loss=0.004962068363013315, val_mse=9.519409832482331e-05, lr=0.0001
Epoch   19: loss=0.004721051287453841, mse=9.172801190054029e-05, val_loss=0.004368666927922213, val_mse=8.96755973820617e-05, lr=0.0001
Epoch   20: loss=0.004423341698493045, mse=8.878177413344692e-05, val_loss=0.004363590755112879, val_mse=8.722076835212399e-05, lr=0.0001
Epoch   21: loss=0.004709836557367022, mse=8.966424549374008e-05, val_loss=0.004608417743643785, val_mse=8.823859307482446e-05, lr=0.0001
Epoch   22: loss=0.004393488563136843, mse=8.629673662184242e-05, val_loss=0.00423530304974097, val_mse=8.591215697358375e-05, lr=0.0001
Epoch   23: loss=0.004434493649242873, mse=8.575318223894314e-05, val_loss=0.004289172823934956, val_mse=8.505435480280696e-05, lr=0.0001
Epoch   24: loss=0.004387730068197576, mse=8.459109173739945e-05, val_loss=0.004280996249999279, val_mse=8.532959919860692e-05, lr=0.0001
Epoch   25: loss=0.003997065923468297, mse=8.066329497434097e-05, val_loss=0.004167713686131096, val_mse=8.004580900970796e-05, lr=0.0001
Epoch   26: loss=0.004299801296291191, mse=8.073750157898868e-05, val_loss=0.004107758491626982, val_mse=8.105541085102021e-05, lr=0.0001
Epoch   27: loss=0.003796016370088562, mse=7.5706439475606e-05, val_loss=0.0037455598667327605, val_mse=7.368984079635454e-05, lr=0.0001
Epoch   28: loss=0.003804480360751968, mse=7.300335740026026e-05, val_loss=0.003566529796206069, val_mse=7.120277542220845e-05, lr=0.0001
Epoch   29: loss=0.003887668345874596, mse=7.049109015573314e-05, val_loss=0.0035033112390062274, val_mse=6.821289553976992e-05, lr=0.0001
Epoch   30: loss=0.003872578239078992, mse=6.647583854975247e-05, val_loss=0.003583831209388952, val_mse=6.34435355044001e-05, lr=0.0001
Epoch   31: loss=0.0036535122795419263, mse=6.0476539958327164e-05, val_loss=0.0035629954367550885, val_mse=5.9207915572263564e-05, lr=0.0001
Epoch   32: loss=0.0035850469599985136, mse=5.521895150929096e-05, val_loss=0.003402892403848879, val_mse=5.367653473485588e-05, lr=0.0001
Epoch   33: loss=0.0035713226789567953, mse=5.0176309759058615e-05, val_loss=0.0033128999595122104, val_mse=4.785683469555696e-05, lr=0.0001
Epoch   34: loss=0.003364226029765221, mse=4.483734343974742e-05, val_loss=0.0030471526141114093, val_mse=4.227448462913311e-05, lr=0.0001
Epoch   35: loss=0.002895660463042833, mse=3.920784203048581e-05, val_loss=0.002746518358475117, val_mse=3.699983492756724e-05, lr=0.0001
Epoch   36: loss=0.002821497327340608, mse=3.599585332515967e-05, val_loss=0.0028980358630114815, val_mse=3.465830770672314e-05, lr=0.0001
Epoch   37: loss=0.0030573661088107587, mse=3.518209731059205e-05, val_loss=0.0031412559215940774, val_mse=3.649673378865426e-05, lr=0.0001
Epoch   38: loss=0.002726618731478353, mse=3.233145149659104e-05, val_loss=0.002731172575136431, val_mse=3.151828370678223e-05, lr=0.0001
Epoch   39: loss=0.0030292087661781025, mse=3.2885071458410133e-05, val_loss=0.002728641190012984, val_mse=3.165730158039046e-05, lr=0.0001
Epoch   40: loss=0.0029067508635540586, mse=3.138057000456852e-05, val_loss=0.003022423029507897, val_mse=3.1191695090569955e-05, lr=0.0001
Epoch   41: loss=0.0025757211420651474, mse=2.9103360147876696e-05, val_loss=0.0022466159465438216, val_mse=2.715186514444689e-05, lr=0.0001
Epoch   42: loss=0.0027420489828586508, mse=2.9782531709638385e-05, val_loss=0.002708868903374791, val_mse=2.8993517471583645e-05, lr=0.0001
Epoch   43: loss=0.0027431641455494095, mse=2.839483547040886e-05, val_loss=0.0028174188177134893, val_mse=2.8175041175199332e-05, lr=0.0001
Epoch   44: loss=0.00254220843462687, mse=2.6461588608368744e-05, val_loss=0.002317152896620319, val_mse=2.5465077923342852e-05, lr=0.0001
Epoch   45: loss=0.002680553646561154, mse=2.6320238675579702e-05, val_loss=0.002461345799168328, val_mse=2.5020854768825498e-05, lr=0.0001
Epoch   46: loss=0.002587392678253699, mse=2.518601849496883e-05, val_loss=0.002956003933240159, val_mse=2.6652900418501913e-05, lr=0.0001
Epoch   47: loss=0.0024511679501999064, mse=2.4153886533550117e-05, val_loss=0.0031424776559953616, val_mse=2.764408046016191e-05, lr=0.0001
Epoch    47: reducing learning rate of group 0 to 8.5000e-05.
Epoch   48: loss=0.0020750192017979265, mse=2.236086517459936e-05, val_loss=0.0020497369456707556, val_mse=2.193604102174321e-05, lr=8.5e-05
Epoch   49: loss=0.002347396434127023, mse=2.3019507210350195e-05, val_loss=0.0020815364435930123, val_mse=2.1927028571244903e-05, lr=8.5e-05
Epoch   50: loss=0.002270022036666333, mse=2.249792885387394e-05, val_loss=0.0022242028841420133, val_mse=2.210636956379104e-05, lr=8.5e-05
Epoch   51: loss=0.0020218063674936073, mse=2.1346870217596793e-05, val_loss=0.0019658795477066056, val_mse=2.0728808011272276e-05, lr=8.5e-05
Epoch   52: loss=0.002150927358165567, mse=2.171138991351235e-05, val_loss=0.0025717172249480093, val_mse=2.3411133824613483e-05, lr=8.5e-05
Epoch   53: loss=0.0023243791786348536, mse=2.19723257158439e-05, val_loss=0.0020996594458705335, val_mse=2.10172730671203e-05, lr=8.5e-05
Epoch   54: loss=0.002179373942949125, mse=2.124088837392551e-05, val_loss=0.0026307352328146373, val_mse=2.315455066890962e-05, lr=8.5e-05
Epoch   55: loss=0.0022047766398941566, mse=2.1210628268511064e-05, val_loss=0.002178348937960769, val_mse=2.0719489601233744e-05, lr=8.5e-05
Epoch   56: loss=0.002176599471312479, mse=2.0928968680211595e-05, val_loss=0.0024017667211415476, val_mse=2.1405144140462965e-05, lr=8.5e-05
Epoch   57: loss=0.002063948577035756, mse=2.0385985642056537e-05, val_loss=0.0019415960741270002, val_mse=1.9516125184290977e-05, lr=8.5e-05
Epoch   58: loss=0.002075250480605459, mse=2.0312025979458183e-05, val_loss=0.002128327628102797, val_mse=2.0058524398468046e-05, lr=8.5e-05
Epoch   59: loss=0.002040623177973504, mse=1.9842555058730414e-05, val_loss=0.0019542493670118488, val_mse=1.9232911599173292e-05, lr=8.5e-05
Epoch   60: loss=0.0019178296863702276, mse=1.9164351764961182e-05, val_loss=0.0018367126446652033, val_mse=1.8595964568926123e-05, lr=8.5e-05
Epoch   61: loss=0.0021705193227166716, mse=1.9728410186015774e-05, val_loss=0.0019166206141785352, val_mse=1.8505301006260705e-05, lr=8.5e-05
Epoch   62: loss=0.001968181752204361, mse=1.862894777137243e-05, val_loss=0.0018540664533580843, val_mse=1.7921038320473842e-05, lr=8.5e-05
Epoch   63: loss=0.0019861099889976427, mse=1.8324308232840987e-05, val_loss=0.0015980033219422601, val_mse=1.666980774790582e-05, lr=8.5e-05
Epoch   64: loss=0.001770328472496984, mse=1.7202524939255305e-05, val_loss=0.001772704738346008, val_mse=1.671291827826705e-05, lr=8.5e-05
Epoch   65: loss=0.0018028551650779636, mse=1.659020739744127e-05, val_loss=0.002198288252089434, val_mse=1.7315681299522843e-05, lr=8.5e-05
Epoch   66: loss=0.0020762157936375482, mse=1.6426438308939658e-05, val_loss=0.0019326389562232557, val_mse=1.5333927117431258e-05, lr=8.5e-05
Epoch   67: loss=0.0019224398070614983, mse=1.503408527217684e-05, val_loss=0.0017613007186026625, val_mse=1.3721794139770452e-05, lr=8.5e-05
Epoch   68: loss=0.0017439642221051882, mse=1.3176405517036296e-05, val_loss=0.0019609179892055902, val_mse=1.322657061941832e-05, lr=8.5e-05
Epoch   69: loss=0.0019247162466063791, mse=1.1224514800788717e-05, val_loss=0.0015926324619701313, val_mse=8.980838718954807e-06, lr=8.5e-05
Epoch   70: loss=0.001649842835621258, mse=7.89375514827357e-06, val_loss=0.002027922048387197, val_mse=7.87256736447035e-06, lr=8.5e-05
Epoch   71: loss=0.0017310042846355891, mse=6.073837474547615e-06, val_loss=0.0015922568624033337, val_mse=5.036837315352314e-06, lr=8.5e-05
Epoch   72: loss=0.0013528174044855072, mse=4.185467055296192e-06, val_loss=0.0012570464823649494, val_mse=3.746458548622878e-06, lr=8.5e-05
Epoch   73: loss=0.0016165019489141742, mse=5.621718946152489e-06, val_loss=0.001383101511516907, val_mse=4.299408338398449e-06, lr=8.5e-05
Epoch   74: loss=0.0014515301999410675, mse=4.5550415183651886e-06, val_loss=0.001091628067462265, val_mse=3.1054193876174862e-06, lr=8.5e-05
Epoch   75: loss=0.0014793124447856805, mse=4.470542377578479e-06, val_loss=0.001402782416766407, val_mse=4.2173327372687025e-06, lr=8.5e-05
Epoch   76: loss=0.0013676611449263735, mse=3.989058305801621e-06, val_loss=0.0014728799716797454, val_mse=4.172784700051773e-06, lr=8.5e-05
Epoch   77: loss=0.0013663122857115859, mse=3.9011026604943984e-06, val_loss=0.0016252762985732088, val_mse=4.629609846809923e-06, lr=8.5e-05
Epoch   78: loss=0.0014064523759890043, mse=4.030984528123486e-06, val_loss=0.0013468795311522353, val_mse=3.771969330950294e-06, lr=8.5e-05
Epoch   79: loss=0.0013617642744721764, mse=3.8257686915381004e-06, val_loss=0.0011360157330642389, val_mse=3.0132205098798605e-06, lr=8.5e-05
Epoch   80: loss=0.0013676309898788149, mse=3.848226289904749e-06, val_loss=0.0012021065670625788, val_mse=3.3256702543446946e-06, lr=8.5e-05
Epoch    80: reducing learning rate of group 0 to 7.2250e-05.
Epoch   81: loss=0.001190948558712168, mse=3.2822129985975526e-06, val_loss=0.0009211655804468584, val_mse=2.4831451443582584e-06, lr=7.225000000000001e-05
Epoch   82: loss=0.0009977052448161894, mse=2.6386158445354878e-06, val_loss=0.0009701459366382865, val_mse=2.5393349385428616e-06, lr=7.225000000000001e-05
Epoch   83: loss=0.0010107075964245129, mse=2.6392064894001517e-06, val_loss=0.001033544429291598, val_mse=2.678947790814103e-06, lr=7.225000000000001e-05
Epoch   84: loss=0.0012229365181848071, mse=3.4012236281115677e-06, val_loss=0.0010240086664903406, val_mse=2.6292589860183125e-06, lr=7.225000000000001e-05
Epoch   85: loss=0.0012636340288529296, mse=3.3627499345067634e-06, val_loss=0.0012873009843946182, val_mse=3.5532936952401855e-06, lr=7.225000000000001e-05
Epoch   86: loss=0.0012096165265424966, mse=3.168903550158316e-06, val_loss=0.0008576420947389794, val_mse=2.2080389794566654e-06, lr=7.225000000000001e-05
Epoch   87: loss=0.0009350529040737463, mse=2.3719074127807576e-06, val_loss=0.0009953801338366485, val_mse=2.5217404733875448e-06, lr=7.225000000000001e-05
Epoch   88: loss=0.0010987240565429129, mse=2.8494673119721707e-06, val_loss=0.001089410464044314, val_mse=2.847933667147288e-06, lr=7.225000000000001e-05
Epoch   89: loss=0.001239098071094613, mse=3.1481792841493467e-06, val_loss=0.00110588095635105, val_mse=2.7323466784062207e-06, lr=7.225000000000001e-05
Epoch   90: loss=0.0010589816643100099, mse=2.644454212833265e-06, val_loss=0.0008272398286410722, val_mse=2.083103331996421e-06, lr=7.225000000000001e-05
Epoch   91: loss=0.0010203589048842763, mse=2.537493009961455e-06, val_loss=0.001117886546416035, val_mse=2.8031063197066644e-06, lr=7.225000000000001e-05
Epoch   92: loss=0.001220830480214226, mse=3.084085163302431e-06, val_loss=0.0012441790095871522, val_mse=3.215360988717321e-06, lr=7.225000000000001e-05
Epoch   93: loss=0.00110000214380198, mse=2.877403277076514e-06, val_loss=0.0011526190556949835, val_mse=2.866748501828469e-06, lr=7.225000000000001e-05
Epoch   94: loss=0.0011434647312025103, mse=2.9777317605204487e-06, val_loss=0.0010490169679300693, val_mse=2.543286015431254e-06, lr=7.225000000000001e-05
Epoch   95: loss=0.001202693132095373, mse=3.0442232163160944e-06, val_loss=0.0009417316236398916, val_mse=2.3560029842277767e-06, lr=7.225000000000001e-05
Epoch   96: loss=0.0010534190346705118, mse=2.55988776210399e-06, val_loss=0.0009134822459697045, val_mse=2.1513187608862234e-06, lr=7.225000000000001e-05
Epoch    96: reducing learning rate of group 0 to 6.1413e-05.
Epoch   97: loss=0.0008450521218210046, mse=2.025817633261493e-06, val_loss=0.0008862098454510125, val_mse=2.0713683119127734e-06, lr=6.141250000000001e-05
Epoch   98: loss=0.0008498296947419498, mse=2.019777868568898e-06, val_loss=0.0008148358076893896, val_mse=1.952030612108025e-06, lr=6.141250000000001e-05
Epoch   99: loss=0.001026621017752187, mse=2.4534036710707264e-06, val_loss=0.0009807528261019296, val_mse=2.3668277537108293e-06, lr=6.141250000000001e-05
Epoch  100: loss=0.0009291677884468138, mse=2.20709584876737e-06, val_loss=0.0007927415539480505, val_mse=1.8798482475552794e-06, lr=6.141250000000001e-05

#########################################
TorchScript: 
 def forward(self,
    x: Tensor) -> Tensor:
  layer_lstm1 = self.layer_lstm1
  out, _0, = (layer_lstm1).forward__0(x, None, )
  layer_lstm2 = self.layer_lstm2
  out0, _1, = (layer_lstm2).forward__0(out, None, )
  layer_lstm3 = self.layer_lstm3
  _2 = (layer_lstm3).forward__0(out0, None, )
  out1, _3, = _2
  _4 = torch.select(torch.slice(out1), 1, -1)
  out2 = torch.slice(_4, 1)
  layer_output = self.layer_output
  return (layer_output).forward(out2, )
 
 graph(%self : __torch__.utils.LSTMNetwork,
      %x.1 : Tensor):
  %27 : int = prim::Constant[value=1]() # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/T4/utils.py:95:14
  %26 : int = prim::Constant[value=0]() # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/T4/utils.py:95:14
  %25 : int = prim::Constant[value=-1]() # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/T4/utils.py:95:21
  %4 : NoneType = prim::Constant()
  %layer_lstm1 : __torch__.torch.nn.modules.rnn.LSTM = prim::GetAttr[name="layer_lstm1"](%self)
  %5 : (Tensor, (Tensor, Tensor)) = prim::CallMethod[name="forward__0"](%layer_lstm1, %x.1, %4) # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/T4/utils.py:92:17
  %out.1 : Tensor, %7 : (Tensor, Tensor) = prim::TupleUnpack(%5)
  %layer_lstm2 : __torch__.torch.nn.modules.rnn.___torch_mangle_0.LSTM = prim::GetAttr[name="layer_lstm2"](%self)
  %11 : (Tensor, (Tensor, Tensor)) = prim::CallMethod[name="forward__0"](%layer_lstm2, %out.1, %4) # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/T4/utils.py:93:17
  %out.5 : Tensor, %13 : (Tensor, Tensor) = prim::TupleUnpack(%11)
  %layer_lstm3 : __torch__.torch.nn.modules.rnn.___torch_mangle_1.LSTM = prim::GetAttr[name="layer_lstm3"](%self)
  %17 : (Tensor, (Tensor, Tensor)) = prim::CallMethod[name="forward__0"](%layer_lstm3, %out.5, %4) # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/T4/utils.py:94:17
  %out.9 : Tensor, %19 : (Tensor, Tensor) = prim::TupleUnpack(%17)
  %30 : Tensor = aten::slice(%out.9, %26, %4, %4, %27) # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/T4/utils.py:95:14
  %32 : Tensor = aten::select(%30, %27, %25) # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/T4/utils.py:95:14
  %out.13 : Tensor = aten::slice(%32, %27, %4, %4, %27) # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/T4/utils.py:95:14
  %layer_output : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_output"](%self)
  %out.17 : Tensor = prim::CallMethod[name="forward"](%layer_output, %out.13) # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/T4/utils.py:98:14
  return (%out.17)

Save the model's TorchScript to model_scripted.pt

