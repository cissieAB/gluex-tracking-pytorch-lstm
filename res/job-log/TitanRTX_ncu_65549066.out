+ srun nvidia-smi
Thu Oct  6 16:44:05 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA TITAN RTX    Off  | 00000000:1B:00.0 Off |                  N/A |
| 41%   29C    P8     1W / 280W |      0MiB / 24220MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
+ env
SLURM_NODELIST=sciml1901
REMOTEHOST=xmei-lpt-wifi.jlab.org
SLURM_LUSTRE_JOB_ID=sciml1901,xmei,65549066
SLURM_JOB_NAME=lstm-train
XDG_SESSION_ID=261024
SLURMD_NODENAME=sciml1901
SLURM_TOPOLOGY_ADDR=sciml1901
HOSTNAME=sciml1901
SLURM_PRIO_PROCESS=0
SLURM_NODE_ALIASES=(null)
HOST=ifarm1901.jlab.org
TERM=xterm-256color
SHELL=/bin/tcsh
SLURM_JOB_QOS=normal
TMPDIR=/tmp
SLURM_TOPOLOGY_ADDR_PATTERN=node
SSH_CLIENT=129.57.82.157 36798 22
OSNAME=Linux
QTDIR=/usr/lib64/qt-3.3
SLURM_JOB_GPUS=0
QTINC=/usr/lib64/qt-3.3/include
SSH_TTY=/dev/pts/216
SLURM_MEM_PER_CPU=4000
QT_GRAPHICSSYSTEM_CHECKED=1
ROCR_VISIBLE_DEVICES=0
SLURM_NNODES=1
GROUP=ITD
USER=xmei
LS_COLORS=rs=0:di=38;5;27:ln=38;5;51:mh=44;38;5;15:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=05;48;5;232;38;5;15:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;34:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.tzo=38;5;9:*.t7z=38;5;9:*.zip=38;5;9:*.z=38;5;9:*.Z=38;5;9:*.dz=38;5;9:*.gz=38;5;9:*.lrz=38;5;9:*.lz=38;5;9:*.lzo=38;5;9:*.xz=38;5;9:*.bz2=38;5;9:*.bz=38;5;9:*.tbz=38;5;9:*.tbz2=38;5;9:*.tz=38;5;9:*.deb=38;5;9:*.rpm=38;5;9:*.jar=38;5;9:*.war=38;5;9:*.ear=38;5;9:*.sar=38;5;9:*.rar=38;5;9:*.alz=38;5;9:*.ace=38;5;9:*.zoo=38;5;9:*.cpio=38;5;9:*.7z=38;5;9:*.rz=38;5;9:*.cab=38;5;9:*.jpg=38;5;13:*.jpeg=38;5;13:*.gif=38;5;13:*.bmp=38;5;13:*.pbm=38;5;13:*.pgm=38;5;13:*.ppm=38;5;13:*.tga=38;5;13:*.xbm=38;5;13:*.xpm=38;5;13:*.tif=38;5;13:*.tiff=38;5;13:*.png=38;5;13:*.svg=38;5;13:*.svgz=38;5;13:*.mng=38;5;13:*.pcx=38;5;13:*.mov=38;5;13:*.mpg=38;5;13:*.mpeg=38;5;13:*.m2v=38;5;13:*.mkv=38;5;13:*.webm=38;5;13:*.ogm=38;5;13:*.mp4=38;5;13:*.m4v=38;5;13:*.mp4v=38;5;13:*.vob=38;5;13:*.qt=38;5;13:*.nuv=38;5;13:*.wmv=38;5;13:*.asf=38;5;13:*.rm=38;5;13:*.rmvb=38;5;13:*.flc=38;5;13:*.avi=38;5;13:*.fli=38;5;13:*.flv=38;5;13:*.gl=38;5;13:*.dl=38;5;13:*.xcf=38;5;13:*.xwd=38;5;13:*.yuv=38;5;13:*.cgm=38;5;13:*.emf=38;5;13:*.axv=38;5;13:*.anx=38;5;13:*.ogv=38;5;13:*.ogx=38;5;13:*.aac=38;5;45:*.au=38;5;45:*.flac=38;5;45:*.mid=38;5;45:*.midi=38;5;45:*.mka=38;5;45:*.mp3=38;5;45:*.mpc=38;5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.axa=38;5;45:*.oga=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:
SLURM_JOBID=65549066
HOSTTYPE=x86_64-linux
COLUMNS=110
SLURM_TASKS_PER_NODE=1
MAIL=/var/spool/mail/xmei
PATH=/usr/lib64/qt-3.3/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin
SLURM_WORKING_CLUSTER=scicomp:enpslurm21:6817:9216:101
SLURM_CONF=/etc/slurm/slurm.conf
SLURM_JOB_ID=65549066
SLURM_CPUS_PER_TASK=8
SLURM_JOB_USER=xmei
PWD=/home/xmei/projects/gluex-tracking-pytorch-lstm/python
CUDA_VISIBLE_DEVICES=0
LANG=en_US.UTF-8
MODULEPATH=/usr/share/Modules/modulefiles:/etc/modulefiles
SLURM_JOB_UID=11066
LOADEDMODULES=
KDEDIRS=/usr
SLURM_NODEID=0
SLURM_SUBMIT_DIR=/w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/python
SLURM_TASK_PID=173914
LINES=24
SLURM_CPUS_ON_NODE=8
SLURM_PROCID=0
ENVIRONMENT=BATCH
SLURM_JOB_NODELIST=sciml1901
PERL_HOMEDIR=0
SHLVL=2
HOME=/home/xmei
SLURM_LOCALID=0
OSTYPE=linux
SLURM_JOB_GID=761
SLURM_JOB_CPUS_PER_NODE=8
SLURM_CLUSTER_NAME=scicomp
SLURM_GTIDS=0
SLURM_SUBMIT_HOST=ifarm1901.jlab.org
SLURM_JOB_PARTITION=gpu
VENDOR=unknown
MACHTYPE=x86_64
LOGNAME=xmei
QTLIB=/usr/lib64/qt-3.3/lib
CVS_RSH=ssh
GPU_DEVICE_ORDINAL=0
SLURM_JOB_ACCOUNT=epsci
SSH_CONNECTION=129.57.82.157 36798 129.57.70.17 22
SLURM_JOB_NUM_NODES=1
MODULESHOME=/usr/share/Modules
LESSOPEN=||/usr/bin/lesspipe.sh %s
XDG_RUNTIME_DIR=/run/user/11066
OSREL=3.10.0-1160.71.1.el7.x86_64
QT_PLUGIN_PATH=/usr/lib64/kde4/plugins:/usr/lib/kde4/plugins
_=/usr/bin/env
+ pwd
/home/xmei/projects/gluex-tracking-pytorch-lstm/python
+ source /etc/profile.d/modules.sh
++++ /bin/ps -p 173914 -ocomm=
+++ /bin/basename slurm_script
++ shell=slurm_script
++ '[' -f /usr/share/Modules/init/slurm_script ']'
++ . /usr/share/Modules/init/sh
+++ MODULESHOME=/usr/share/Modules
+++ export MODULESHOME
+++ '[' '' = '' ']'
+++ LOADEDMODULES=
+++ export LOADEDMODULES
+++ '[' /usr/share/Modules/modulefiles:/etc/modulefiles = '' ']'
+ module load python3
++ /usr/bin/modulecmd sh load python3
ModuleCmd_Load.c(213):ERROR:105: Unable to locate a modulefile for 'python3'
+ eval
+ pip3 install pandas numpy sklearn torch matplotlib
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: pandas in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (1.1.5)
Requirement already satisfied: numpy in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (1.19.5)
Requirement already satisfied: sklearn in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (0.0)
Requirement already satisfied: torch in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (1.10.2)
Requirement already satisfied: matplotlib in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (3.3.4)
Requirement already satisfied: python-dateutil>=2.7.3 in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from pandas) (2.8.2)
Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas) (2021.3)
Requirement already satisfied: scikit-learn in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from sklearn) (0.24.2)
Requirement already satisfied: dataclasses in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from torch) (0.8)
Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/site-packages (from torch) (4.1.1)
Requirement already satisfied: kiwisolver>=1.0.1 in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from matplotlib) (1.3.1)
Requirement already satisfied: cycler>=0.10 in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from matplotlib) (0.11.0)
Requirement already satisfied: pillow>=6.2.0 in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from matplotlib) (8.4.0)
Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from matplotlib) (3.0.9)
Requirement already satisfied: six>=1.5 in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)
Requirement already satisfied: threadpoolctl>=2.0.0 in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from scikit-learn->sklearn) (3.1.0)
Requirement already satisfied: scipy>=0.19.1 in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.5.4)
Requirement already satisfied: joblib>=0.11 in /w/epsci-sciwork18/xmei/jhub/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.1.0)
+ srun python3 Simplified_LSTM.py
Torch version 1.10.2+cu102
Training on cuda device
Epoch    1: loss=0.05707158839800156, mse=0.01738798028633421, val_loss=0.00969026151662816, val_mse=0.0002705020924415563, lr=0.0001
Epoch    2: loss=0.008860738302070287, mse=0.0002548022434505633, val_loss=0.00817000203407184, val_mse=0.0002494692961809948, lr=0.0001
Epoch    3: loss=0.007898937687021547, mse=0.00024059346963852942, val_loss=0.008220388731641777, val_mse=0.00024509002298622105, lr=0.0001
Epoch    4: loss=0.007911525094577692, mse=0.00023972266543504387, val_loss=0.007857774064453775, val_mse=0.00023890196235737353, lr=0.0001
Epoch    5: loss=0.0078017947118662195, mse=0.00023731937305526842, val_loss=0.007471621081593537, val_mse=0.00023407819199303354, lr=0.0001
Epoch    6: loss=0.007603929427574323, mse=0.00023348006457998285, val_loss=0.007098521401070101, val_mse=0.00022768824201331703, lr=0.0001
Epoch    7: loss=0.0069522247850750014, mse=0.00022214112663453806, val_loss=0.00728204901165822, val_mse=0.00022144349684242504, lr=0.0001
Epoch    8: loss=0.006756255657419181, mse=0.00020913877219590844, val_loss=0.006709894983617472, val_mse=0.0001997355584276529, lr=0.0001
Epoch    9: loss=0.006841808052749446, mse=0.00019087863660409104, val_loss=0.006949860041526339, val_mse=0.00018155997124433417, lr=0.0001
Epoch   10: loss=0.006688085300503281, mse=0.0001691767177602521, val_loss=0.006263388479853038, val_mse=0.00015616380962794134, lr=0.0001
Epoch   11: loss=0.006220639842355613, mse=0.00014831080582499613, val_loss=0.0065204234788480584, val_mse=0.0001450229109058588, lr=0.0001
Epoch   12: loss=0.005941248115371256, mse=0.00013998547772478412, val_loss=0.005520179711345873, val_mse=0.00013571639309559492, lr=0.0001
Epoch   13: loss=0.005960550504043854, mse=0.00014062442673235272, val_loss=0.0055301489928769095, val_mse=0.00013686356189551602, lr=0.0001
Epoch   14: loss=0.005736407407902294, mse=0.00014008668373756715, val_loss=0.005811424540670366, val_mse=0.00014259343555080484, lr=0.0001
Epoch   15: loss=0.005555657431769558, mse=0.00013547089078498517, val_loss=0.005540384020324111, val_mse=0.00013227281292365418, lr=0.0001
Epoch   16: loss=0.005406776779336569, mse=0.00013123804737967407, val_loss=0.005300988395481001, val_mse=0.00012937708150282187, lr=0.0001
Epoch   17: loss=0.00536838021090491, mse=0.00012845711827887165, val_loss=0.00638499949392518, val_mse=0.00014069798174186078, lr=0.0001
Epoch   18: loss=0.005572495886621258, mse=0.00012715379119916081, val_loss=0.005701325847935179, val_mse=0.00012594270084735423, lr=0.0001
Epoch   19: loss=0.005476073460676398, mse=0.0001245157318287213, val_loss=0.005163304847037769, val_mse=0.0001205911707889423, lr=0.0001
Epoch   20: loss=0.0053478862261452645, mse=0.00012196757072171191, val_loss=0.005275244839609631, val_mse=0.000122081643982505, lr=0.0001
Epoch   21: loss=0.004881886109061172, mse=0.00011706598282750145, val_loss=0.00506884227126865, val_mse=0.00011637417675604323, lr=0.0001
Epoch   22: loss=0.005078970603962947, mse=0.0001174857569592521, val_loss=0.0048333872630980145, val_mse=0.00011481191039090248, lr=0.0001
Epoch   23: loss=0.0051724112714575405, mse=0.00011606442935504568, val_loss=0.005602448092741747, val_mse=0.00011894888026649538, lr=0.0001
Epoch   24: loss=0.005184364332366855, mse=0.00011417797833559169, val_loss=0.005117650632437255, val_mse=0.00011238099690900048, lr=0.0001
Epoch   25: loss=0.005175984950349145, mse=0.0001125117068919415, val_loss=0.004780861170895299, val_mse=0.00010876452913233901, lr=0.0001
Epoch   26: loss=0.004912078920248154, mse=0.00010857673354262546, val_loss=0.004815227266372777, val_mse=0.00010676413558116908, lr=0.0001
Epoch   27: loss=0.0048908147694935325, mse=0.00010630676571697932, val_loss=0.004967912482857648, val_mse=0.00010973051812436142, lr=0.0001
Epoch   28: loss=0.004480592173743336, mse=0.0001008519637190774, val_loss=0.004480440709457169, val_mse=9.87649175579893e-05, lr=0.0001
Epoch   29: loss=0.004465663235235175, mse=9.757596704778854e-05, val_loss=0.004536101120953542, val_mse=9.510778007760249e-05, lr=0.0001
Epoch   30: loss=0.004637848567288277, mse=9.53428921654175e-05, val_loss=0.004707536350872078, val_mse=9.345134289834058e-05, lr=0.0001
Epoch   31: loss=0.004763935452267047, mse=9.289913059874863e-05, val_loss=0.004434402026518812, val_mse=8.900366369045058e-05, lr=0.0001
Epoch   32: loss=0.004614581180775301, mse=8.916683311130037e-05, val_loss=0.004693770335821092, val_mse=9.028060680173119e-05, lr=0.0001
Epoch   33: loss=0.004485265882107017, mse=8.575191645357497e-05, val_loss=0.004169510509129578, val_mse=8.178887836545868e-05, lr=0.0001
Epoch   34: loss=0.004232471785486006, mse=8.144099902362956e-05, val_loss=0.0043218432363667, val_mse=7.975323862593667e-05, lr=0.0001
Epoch   35: loss=0.0041892394276364115, mse=7.821927438634232e-05, val_loss=0.004178743528474327, val_mse=7.609470648115829e-05, lr=0.0001
Epoch   36: loss=0.0042371412392783435, mse=7.560152311446971e-05, val_loss=0.004705162886777571, val_mse=7.728356910000009e-05, lr=0.0001
Epoch   37: loss=0.0045427684834539765, mse=7.518902019094154e-05, val_loss=0.004816643568551065, val_mse=8.126495883069346e-05, lr=0.0001
Epoch   38: loss=0.004456903533638938, mse=7.042352529232971e-05, val_loss=0.004524138432695925, val_mse=6.907338300832297e-05, lr=0.0001
Epoch   39: loss=0.004198949575177668, mse=6.516597327140388e-05, val_loss=0.004484229159154408, val_mse=6.479418169851972e-05, lr=0.0001
Epoch    39: reducing learning rate of group 0 to 8.5000e-05.
Epoch   40: loss=0.004020333683747353, mse=5.964022702117211e-05, val_loss=0.0041811492309978045, val_mse=5.862817421417014e-05, lr=8.5e-05
Epoch   41: loss=0.003849713970108251, mse=5.205674906167879e-05, val_loss=0.0035445727646506377, val_mse=4.588906110552196e-05, lr=8.5e-05
Epoch   42: loss=0.0037421604365055613, mse=4.387341309847435e-05, val_loss=0.003599500250905413, val_mse=3.911160823041476e-05, lr=8.5e-05
Epoch   43: loss=0.0034584239732953002, mse=3.697034780118209e-05, val_loss=0.0032229730819636495, val_mse=3.385179381294262e-05, lr=8.5e-05
Epoch   44: loss=0.0032922361669902897, mse=3.297162615268096e-05, val_loss=0.0030425135200875023, val_mse=3.0459345854229896e-05, lr=8.5e-05
Epoch   45: loss=0.0029792358804550693, mse=2.9541867930688732e-05, val_loss=0.0029206796557213087, val_mse=2.8517872559112554e-05, lr=8.5e-05
Epoch   46: loss=0.0030395342699787533, mse=2.9177868694410614e-05, val_loss=0.0036834767388720666, val_mse=3.2572183283136456e-05, lr=8.5e-05
Epoch   47: loss=0.0032170735253418746, mse=2.9424711815072425e-05, val_loss=0.0029124490226631315, val_mse=2.7498728030165074e-05, lr=8.5e-05
Epoch   48: loss=0.002995580570101632, mse=2.7830591180650748e-05, val_loss=0.0029667117968696457, val_mse=2.709964369137067e-05, lr=8.5e-05
Epoch   49: loss=0.0029701170513973672, mse=2.7217944596769663e-05, val_loss=0.002733522522452321, val_mse=2.6026536867455925e-05, lr=8.5e-05
Epoch   50: loss=0.0028859174407645707, mse=2.646363334315883e-05, val_loss=0.0030732914360073983, val_mse=2.8072177693749243e-05, lr=8.5e-05
Epoch   51: loss=0.0028576481566266057, mse=2.5950779337907587e-05, val_loss=0.002556152496861552, val_mse=2.4069746791049785e-05, lr=8.5e-05
Epoch   52: loss=0.0027670112643496925, mse=2.5062030450167796e-05, val_loss=0.0027552729201250387, val_mse=2.4510336395153606e-05, lr=8.5e-05
Epoch   53: loss=0.0028568117605854513, mse=2.5063705974116775e-05, val_loss=0.0028284264077208018, val_mse=2.457952307227052e-05, lr=8.5e-05
Epoch   54: loss=0.002788756916386056, mse=2.4592726683175962e-05, val_loss=0.002657218969230888, val_mse=2.3453201005778513e-05, lr=8.5e-05
Epoch   55: loss=0.0024698935650144145, mse=2.2419863614406177e-05, val_loss=0.002274627450047736, val_mse=2.1592917405274312e-05, lr=8.5e-05
Epoch   56: loss=0.0023826632710117175, mse=2.184700270453741e-05, val_loss=0.0022476308990580928, val_mse=2.1134982781116478e-05, lr=8.5e-05
Epoch   57: loss=0.0023475631873125114, mse=2.1403001408913108e-05, val_loss=0.002536523975414247, val_mse=2.213544158091697e-05, lr=8.5e-05
Epoch   58: loss=0.0023153366314899353, mse=2.1018427013978672e-05, val_loss=0.002357387167279674, val_mse=2.105666069741108e-05, lr=8.5e-05
Epoch   59: loss=0.0022856314644258136, mse=2.0671840251369685e-05, val_loss=0.002310937323538583, val_mse=2.099940372595941e-05, lr=8.5e-05
Epoch   60: loss=0.002249998262172399, mse=2.031245442696631e-05, val_loss=0.0022780329423763945, val_mse=2.0366930472537518e-05, lr=8.5e-05
Epoch   61: loss=0.002222504845563679, mse=2.0014548552067036e-05, val_loss=0.002251542121193158, val_mse=2.007209920574336e-05, lr=8.5e-05
Epoch   62: loss=0.0022365294448899696, mse=1.99416453827617e-05, val_loss=0.0027632357665749382, val_mse=2.2294921514840374e-05, lr=8.5e-05
Epoch    62: reducing learning rate of group 0 to 7.2250e-05.
Epoch   63: loss=0.0023388654173420744, mse=2.0219255760859364e-05, val_loss=0.002289951331400979, val_mse=1.9739553105849084e-05, lr=7.225000000000001e-05
Epoch   64: loss=0.002360930318572599, mse=2.027275734455484e-05, val_loss=0.0020803404208520108, val_mse=1.898927740448736e-05, lr=7.225000000000001e-05
Epoch   65: loss=0.0022994559717698614, mse=1.990681230347081e-05, val_loss=0.002280604189254725, val_mse=1.974528710234913e-05, lr=7.225000000000001e-05
Epoch   66: loss=0.002321932907359837, mse=1.9805414272540244e-05, val_loss=0.0022570811511605864, val_mse=1.9710520433429434e-05, lr=7.225000000000001e-05
Epoch   67: loss=0.0021186623887659937, mse=1.8904473764507765e-05, val_loss=0.0020747507742978986, val_mse=1.8601800238995163e-05, lr=7.225000000000001e-05
Epoch   68: loss=0.002095921894741303, mse=1.8673824463646563e-05, val_loss=0.002369677511430867, val_mse=1.964700029488296e-05, lr=7.225000000000001e-05
Epoch   69: loss=0.0023128643048312197, mse=1.934445808780698e-05, val_loss=0.0019643637822212854, val_mse=1.7922483650796057e-05, lr=7.225000000000001e-05
Epoch   70: loss=0.0019767855254175988, mse=1.7875188312684234e-05, val_loss=0.002014248365928654, val_mse=1.806241286225662e-05, lr=7.225000000000001e-05
Epoch   71: loss=0.0019884986405974817, mse=1.7771245267063427e-05, val_loss=0.0020806190990303878, val_mse=1.8080358751116748e-05, lr=7.225000000000001e-05
Epoch   72: loss=0.0019703836677740887, mse=1.756230698364487e-05, val_loss=0.0019491632118647957, val_mse=1.738692678098498e-05, lr=7.225000000000001e-05
Epoch   73: loss=0.0019523514727611214, mse=1.7347644573555868e-05, val_loss=0.0018835416252758403, val_mse=1.705382560456622e-05, lr=7.225000000000001e-05
Epoch   74: loss=0.0019367898926377451, mse=1.7145855072520574e-05, val_loss=0.0019144474557888972, val_mse=1.695980979112276e-05, lr=7.225000000000001e-05
Epoch   75: loss=0.001920705738161961, mse=1.6935052371128433e-05, val_loss=0.002183007337488729, val_mse=1.7839937908211555e-05, lr=7.225000000000001e-05
Epoch   76: loss=0.0019125945175126676, mse=1.6750541591952726e-05, val_loss=0.001962950089557602, val_mse=1.661032931407261e-05, lr=7.225000000000001e-05
Epoch   77: loss=0.0018931664406070998, mse=1.6536830121134547e-05, val_loss=0.0018319817541778596, val_mse=1.610921548838907e-05, lr=7.225000000000001e-05
Epoch   78: loss=0.0018756559741682421, mse=1.6320326734132988e-05, val_loss=0.0018765173301956064, val_mse=1.610004493940853e-05, lr=7.225000000000001e-05
Epoch   79: loss=0.0019402055383926748, mse=1.6335924628671268e-05, val_loss=0.002001527931445775, val_mse=1.6295503945285596e-05, lr=7.225000000000001e-05
Epoch   80: loss=0.0022441879618626645, mse=1.745321181276338e-05, val_loss=0.002106233728356815, val_mse=1.647922216152593e-05, lr=7.225000000000001e-05
Epoch   81: loss=0.0019294675994612298, mse=1.5955010468845723e-05, val_loss=0.0018533390948453484, val_mse=1.5601864653506734e-05, lr=7.225000000000001e-05
Epoch   82: loss=0.0020030476076685713, mse=1.6283376708847675e-05, val_loss=0.0018799175774286267, val_mse=1.5644857192894026e-05, lr=7.225000000000001e-05
Epoch   83: loss=0.001840271660583129, mse=1.547077287219712e-05, val_loss=0.001840873827284778, val_mse=1.5117903345643783e-05, lr=7.225000000000001e-05
Epoch    83: reducing learning rate of group 0 to 6.1413e-05.
Epoch   84: loss=0.0018913580781883565, mse=1.543636622425272e-05, val_loss=0.0018285925281843775, val_mse=1.4815313857786784e-05, lr=6.141250000000001e-05
Epoch   85: loss=0.0017584970877443987, mse=1.4932460071646257e-05, val_loss=0.0016614533492507134, val_mse=1.4461716376074927e-05, lr=6.141250000000001e-05
Epoch   86: loss=0.0017462339779857785, mse=1.478386264696027e-05, val_loss=0.0018537752089520078, val_mse=1.5269497299522795e-05, lr=6.141250000000001e-05
Epoch   87: loss=0.001749582892412804, mse=1.4631597165856118e-05, val_loss=0.0018044179267880896, val_mse=1.4406416259272059e-05, lr=6.141250000000001e-05
Epoch   88: loss=0.0019330125185219214, mse=1.5212880416894727e-05, val_loss=0.0018636319572918924, val_mse=1.5032141938711365e-05, lr=6.141250000000001e-05
Epoch   89: loss=0.0017539694214631155, mse=1.4444530753596381e-05, val_loss=0.001638125990512136, val_mse=1.4041113625619091e-05, lr=6.141250000000001e-05
Epoch   90: loss=0.0017016696580420593, mse=1.4099390895235927e-05, val_loss=0.001740565396265694, val_mse=1.4108292492222751e-05, lr=6.141250000000001e-05
Epoch   91: loss=0.0019066364090647584, mse=1.4539690532754742e-05, val_loss=0.0021213134808158343, val_mse=1.558863543696013e-05, lr=6.141250000000001e-05
Epoch   92: loss=0.0018722527816496697, mse=1.4328651007657533e-05, val_loss=0.0017276401289136095, val_mse=1.3541242675356518e-05, lr=6.141250000000001e-05
Epoch   93: loss=0.0018951954545172204, mse=1.4336257703365176e-05, val_loss=0.00222332925982844, val_mse=1.525923833562471e-05, lr=6.141250000000001e-05
Epoch   94: loss=0.0018576526386681873, mse=1.4033491950861611e-05, val_loss=0.0018953754872779948, val_mse=1.402835840077901e-05, lr=6.141250000000001e-05
Epoch   95: loss=0.001656329431745579, mse=1.3264146207067507e-05, val_loss=0.0017246249358503739, val_mse=1.3331814983692215e-05, lr=6.141250000000001e-05
Epoch    95: reducing learning rate of group 0 to 5.2201e-05.
Epoch   96: loss=0.00160818138032572, mse=1.3000723521340135e-05, val_loss=0.0016165203710865985, val_mse=1.3046903285676498e-05, lr=5.2200625000000005e-05
Epoch   97: loss=0.0016067133520255342, mse=1.284694065532545e-05, val_loss=0.0016967640669249436, val_mse=1.309940197931508e-05, lr=5.2200625000000005e-05
Epoch   98: loss=0.0015972132181114318, mse=1.2666985024706638e-05, val_loss=0.001575258958985785, val_mse=1.2447519035511995e-05, lr=5.2200625000000005e-05
Epoch   99: loss=0.0016604574022004363, mse=1.2664971243369297e-05, val_loss=0.0015582082091973816, val_mse=1.2233354564280332e-05, lr=5.2200625000000005e-05
Epoch  100: loss=0.0015969775277817273, mse=1.2329150776594079e-05, val_loss=0.001678574824108401, val_mse=1.2465972180804216e-05, lr=5.2200625000000005e-05

#########################################
TorchScript: 
 def forward(self,
    x: Tensor) -> Tensor:
  layer_lstm1 = self.layer_lstm1
  out, _0, = (layer_lstm1).forward__0(x, None, )
  layer_lstm2 = self.layer_lstm2
  out0, _1, = (layer_lstm2).forward__0(out, None, )
  layer_lstm3 = self.layer_lstm3
  _2 = (layer_lstm3).forward__0(out0, None, )
  out1, _3, = _2
  _4 = torch.select(torch.slice(out1), 1, -1)
  out2 = torch.slice(_4, 1)
  layer_output = self.layer_output
  return (layer_output).forward(out2, )
 
 graph(%self : __torch__.utils.LSTMNetwork,
      %x.1 : Tensor):
  %27 : int = prim::Constant[value=1]() # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/python/utils.py:95:14
  %26 : int = prim::Constant[value=0]() # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/python/utils.py:95:14
  %25 : int = prim::Constant[value=-1]() # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/python/utils.py:95:21
  %4 : NoneType = prim::Constant()
  %layer_lstm1 : __torch__.torch.nn.modules.rnn.LSTM = prim::GetAttr[name="layer_lstm1"](%self)
  %5 : (Tensor, (Tensor, Tensor)) = prim::CallMethod[name="forward__0"](%layer_lstm1, %x.1, %4) # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/python/utils.py:92:17
  %out.1 : Tensor, %7 : (Tensor, Tensor) = prim::TupleUnpack(%5)
  %layer_lstm2 : __torch__.torch.nn.modules.rnn.___torch_mangle_0.LSTM = prim::GetAttr[name="layer_lstm2"](%self)
  %11 : (Tensor, (Tensor, Tensor)) = prim::CallMethod[name="forward__0"](%layer_lstm2, %out.1, %4) # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/python/utils.py:93:17
  %out.5 : Tensor, %13 : (Tensor, Tensor) = prim::TupleUnpack(%11)
  %layer_lstm3 : __torch__.torch.nn.modules.rnn.___torch_mangle_1.LSTM = prim::GetAttr[name="layer_lstm3"](%self)
  %17 : (Tensor, (Tensor, Tensor)) = prim::CallMethod[name="forward__0"](%layer_lstm3, %out.5, %4) # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/python/utils.py:94:17
  %out.9 : Tensor, %19 : (Tensor, Tensor) = prim::TupleUnpack(%17)
  %30 : Tensor = aten::slice(%out.9, %26, %4, %4, %27) # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/python/utils.py:95:14
  %32 : Tensor = aten::select(%30, %27, %25) # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/python/utils.py:95:14
  %out.13 : Tensor = aten::slice(%32, %27, %4, %4, %27) # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/python/utils.py:95:14
  %layer_output : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_output"](%self)
  %out.17 : Tensor = prim::CallMethod[name="forward"](%layer_output, %out.13) # /w/epsci-sciwork18/xmei/projects/gluex-tracking-pytorch-lstm/python/utils.py:98:14
  return (%out.17)

Save the model's TorchScript to model_scripted.pt

